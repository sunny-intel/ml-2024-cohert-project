{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tMkb9JhZUGsF"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eQx-VIM3Xerw"
      },
      "outputs": [],
      "source": [
        "# Example of a transformer model from scratch using PyTorch: https://github.com/Khaliladib11/Transformer-from-scratch/\n",
        "# Illustration: https://jalammar.github.io/illustrated-transformer/\n",
        "# Mathematical Visualization: https://people.tamu.edu/~sji/classes/attn-slides.pdf\n",
        "# Another good set of diagrams to understand attention: https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\n",
        "#   and https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15zi9dpKJk5J",
        "outputId": "e20b4545-593c-4215-a370-ddcd6ffe7db1"
      },
      "outputs": [],
      "source": [
        "# Tokens to vectors: Token embeddings using word2vec like MLP\n",
        "# [READ] https://discuss.huggingface.co/t/the-inputs-into-bert-are-token-ids-how-do-we-get-the-corresponding-input-token-vectors/11273\n",
        "# Above tokenizer outputs token IDs, which map (internally) to embedding vectors, which could be accessed using https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958\n",
        "# The word to token to embedding is part of the bigger transfomer and is trained alongwith the complete model, instead of using an 'off the shelf' word2vec trained MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v7SsYZBRfPA-"
      },
      "outputs": [],
      "source": [
        "# Only to understand, we define tensors for different aspects\n",
        "\n",
        "D = 6   # Embedding Vector length => # of dimensions in word2vec terms\n",
        "L = 8    # Length of sequence. If tokenized input is shorter, pad it so that resulting length is L\n",
        "\n",
        "# Word embedding tensor - 1 column for each token\n",
        "WE = torch.rand((D, L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41_58rl9hPek"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "# Since transformer doesn't use any recurrence, to provide importance to nearby tokens, positional embedding is added to token embedding.\n",
        "# Refer to attention paper or https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "# TODO: Add code to generate position embedding lookup and add to word embedding?\n",
        "PE = torch.rand((D, L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOE44DXJTw5D",
        "outputId": "36a44d02-dd64-41a0-8979-e03142bc2a87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The input is sum of word and position embeddings\n",
        "I = WE + PE\n",
        "I.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6tqriP1Xdfr",
        "outputId": "a14f764e-af9f-4a55-e87a-87645fccd5b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 6])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The input is linearly transformed (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to create query and key matrices\n",
        "# Essentially, we're transforming each higher dimension (D) embedding into a smaller dimension (M = key-query space dimension) vector, e.g Q = WQ dot I\n",
        "# https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6&t=269s\n",
        "M = 10\n",
        "\n",
        "# TODO: Use\n",
        "WQ = torch.rand((M, D))\n",
        "WK = torch.rand((M, D))\n",
        "\n",
        "WQ.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxm8EzFchPzu",
        "outputId": "4b1de29d-d2e6-4bab-f8d7-e3049ae77f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2.3897, 1.7479, 2.6692, 1.6097, 1.6942, 1.7014, 1.5254, 1.6739],\n",
              "        [3.4826, 2.8201, 5.3857, 3.2962, 3.9728, 2.4400, 4.0953, 3.4223],\n",
              "        [2.3004, 1.5296, 2.9333, 1.5701, 1.9988, 1.6108, 1.8064, 1.7154],\n",
              "        [2.0118, 1.5871, 3.3924, 1.9076, 2.4824, 1.4944, 2.3505, 2.0305],\n",
              "        [2.5622, 1.9513, 3.5046, 1.9457, 2.4713, 1.5285, 2.8915, 2.2452],\n",
              "        [3.0498, 2.0576, 3.6320, 2.3153, 2.4279, 1.9450, 2.5457, 2.4038],\n",
              "        [2.8447, 2.1060, 4.1158, 2.5028, 2.9352, 2.2065, 2.5113, 2.4870],\n",
              "        [3.6852, 2.8830, 4.5143, 2.5416, 3.3001, 2.1887, 4.0459, 2.9172],\n",
              "        [2.4057, 1.9910, 3.1818, 1.7673, 2.3491, 1.7989, 2.2642, 1.8796],\n",
              "        [3.6407, 3.3560, 4.7156, 2.6356, 3.3087, 2.7745, 3.2263, 2.8537]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pytorch way of calculating dot-product\n",
        "Q = torch.einsum('ij, jk -> ik', WQ, I)\n",
        "K = torch.einsum('ij, jk -> ik', WK, I)\n",
        "\n",
        "Q.size()\n",
        "K.size()\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrPRTVCKkOQx",
        "outputId": "d7543420-67bc-4d1c-9658-b1ece1ee1f92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 89.9745,  70.6371, 121.2894,  70.2107,  86.4011,  62.3521,  88.2116,\n",
              "          75.3157],\n",
              "        [ 67.1082,  52.2188,  90.4301,  52.3070,  64.3178,  46.4574,  65.4251,\n",
              "          56.0926],\n",
              "        [112.9106,  87.9641, 152.2493,  88.2319, 108.3713,  78.2108, 110.2667,\n",
              "          94.5058],\n",
              "        [ 64.7450,  50.4106,  87.2677,  50.5651,  62.1189,  44.7724,  63.3186,\n",
              "          54.1950],\n",
              "        [ 78.2327,  60.9336, 105.3769,  61.0377,  74.9949,  54.1628,  76.3433,\n",
              "          65.4064],\n",
              "        [ 56.3546,  43.6808,  75.5455,  43.6546,  53.6566,  38.8353,  54.7700,\n",
              "          46.8999],\n",
              "        [ 87.5789,  68.9833, 118.6289,  68.7970,  84.6466,  60.9697,  86.1163,\n",
              "          73.6193],\n",
              "        [ 72.2746,  56.4820,  97.6490,  56.6189,  69.5675,  50.1244,  70.8019,\n",
              "          60.6220]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Attention: Each column in 'Q' is trying to ask a question, and when we calculate K' dot Q, the closely matching Q and K vectors result in large \"attention\" outputs\n",
        "# Each column in attention matrix A tells how that other tokens in the input attend to this token in the input\n",
        "A = torch.einsum('ij, jk -> ik', torch.transpose(K, 0, 1), Q)\n",
        "\n",
        "# TODO: To avoid later tokens to influence earlier tokens (acausal behaviour), we mask the lower diagonal values in A, so that every key with index higher than query index is set to -INF, so that softmax calculation is not impacted\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5bzOqxonSe5",
        "outputId": "0ae5f52d-2ace-4c59-c9ee-863b84be61d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we perform softmax on A to convert each column of A, to find the most likelihood\n",
        "A = nn.functional.softmax(A / sqrt(M), dim=0)\n",
        "A.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2jk0JtPzrP-",
        "outputId": "22026f91-04b7-44b4-8f1f-889052aa86ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we adjust each input embedding using the self-attention calculated above, to add context information: https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6&t=790s\n",
        "# Conceptually, we use a D X D matrix to generate linearly transformed input embedding, and then dot product with attention A to find required changes in input embedding to add context\n",
        "# Bit it can be more efficiently done by using a product of two weight matrix:\n",
        "\n",
        "WVu = torch.rand((D, M))\n",
        "WVd = torch.rand((M, D))\n",
        "\n",
        "# First, we down-size input embedding from D to M\n",
        "Vd = torch.einsum('ij, jk -> ik', WVd, I)\n",
        "Vd.size()\n",
        "\n",
        "# Then, we up-size Vd back to D space\n",
        "V = torch.einsum('ij, jk -> ik', WVu, Vd)\n",
        "V.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olfgH0nHD3g9",
        "outputId": "c6e07dc2-9ecd-44ea-b5d7-c6a2b39c6940"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, we perform V x A to get change in embeddings: essentially, A gives the weight for each vector, and we average weighted value vectors to get change needed\n",
        "dI = torch.einsum('ij, jk -> ik', V, A)\n",
        "dI.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XMuTcuYbyrB",
        "outputId": "475a8a66-71e2-44e2-b4db-6a0222d31787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# And add dI to input embedding to get transformed output embedding\n",
        "O = I + dI\n",
        "O.size()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
