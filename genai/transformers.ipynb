{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tMkb9JhZUGsF"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eQx-VIM3Xerw"
      },
      "outputs": [],
      "source": [
        "# Example of a transformer model from scratch using PyTorch: https://github.com/Khaliladib11/Transformer-from-scratch/\n",
        "# Illustration: https://jalammar.github.io/illustrated-transformer/\n",
        "# Mathematical Visualization: https://people.tamu.edu/~sji/classes/attn-slides.pdf\n",
        "# Another good set of diagrams to understand attention: https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\n",
        "#   and https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15zi9dpKJk5J",
        "outputId": "e20b4545-593c-4215-a370-ddcd6ffe7db1"
      },
      "outputs": [],
      "source": [
        "# Tokens to vectors: Token embeddings using word2vec like MLP\n",
        "# [READ] https://discuss.huggingface.co/t/the-inputs-into-bert-are-token-ids-how-do-we-get-the-corresponding-input-token-vectors/11273\n",
        "# Above tokenizer outputs token IDs, which map (internally) to embedding vectors, which could be accessed using https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958\n",
        "# The word to token to embedding is part of the bigger transfomer and is trained alongwith the complete model, instead of using an 'off the shelf' word2vec trained MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v7SsYZBRfPA-"
      },
      "outputs": [],
      "source": [
        "# Only to understand, we define tensors for different aspects\n",
        "\n",
        "D = 6   # Embedding Vector length => # of dimensions in word2vec terms\n",
        "L = 8    # Length of sequence. If tokenized input is shorter, pad it so that resulting length is L\n",
        "\n",
        "# Word embedding tensor - 1 column for each token\n",
        "WE = torch.rand((D, L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41_58rl9hPek"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "# Since transformer doesn't use any recurrence, to provide importance to nearby tokens, positional embedding is added to token embedding.\n",
        "# Refer to attention paper or https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "# TODO: Add code to generate position embedding lookup and add to word embedding?\n",
        "PE = torch.rand((D, L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOE44DXJTw5D",
        "outputId": "36a44d02-dd64-41a0-8979-e03142bc2a87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The input is sum of word and position embeddings\n",
        "I = WE + PE\n",
        "I.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6tqriP1Xdfr",
        "outputId": "a14f764e-af9f-4a55-e87a-87645fccd5b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 6])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The input is linearly transformed (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to create query and key matrices\n",
        "# Essentially, we're transforming each higher dimension (D) embedding into a smaller dimension (M = key-query space dimension) vector, e.g Q = WQ dot I\n",
        "# https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6&t=269s\n",
        "M = 10\n",
        "\n",
        "# TODO: Use\n",
        "WQ = torch.rand((M, D))\n",
        "WK = torch.rand((M, D))\n",
        "\n",
        "WQ.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxm8EzFchPzu",
        "outputId": "4b1de29d-d2e6-4bab-f8d7-e3049ae77f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[3.1693, 2.1927, 2.8809, 2.5130, 2.9900, 2.2685, 3.7322, 2.8889],\n",
              "        [2.6887, 2.1459, 2.6003, 2.4136, 2.7068, 2.6250, 3.6825, 2.7982],\n",
              "        [2.9437, 2.6521, 2.5546, 1.9819, 2.7460, 2.2680, 3.2884, 2.6828],\n",
              "        [3.3613, 2.7055, 2.9411, 2.6842, 3.8369, 3.1979, 4.5251, 3.4753],\n",
              "        [2.6514, 2.1662, 2.4622, 2.2088, 2.9289, 2.4920, 3.5213, 2.7258],\n",
              "        [4.5704, 3.4810, 4.4151, 3.7938, 4.2986, 3.6176, 5.5293, 4.3299],\n",
              "        [3.0719, 2.4255, 2.6300, 2.2707, 3.3615, 2.6705, 3.8966, 3.0434],\n",
              "        [1.4318, 1.2466, 1.4742, 1.0371, 1.2894, 1.3181, 1.7367, 1.4644],\n",
              "        [3.7080, 2.8286, 3.8334, 2.8846, 2.8095, 2.5994, 4.0383, 3.3360],\n",
              "        [5.4759, 4.5118, 4.9074, 3.6270, 4.7713, 3.9199, 5.9387, 4.9061]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pytorch way of calculating dot-product\n",
        "Q = torch.einsum('ij, jk -> ik', WQ, I)\n",
        "K = torch.einsum('ij, jk -> ik', WK, I)\n",
        "\n",
        "Q.size()\n",
        "K.size()\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrPRTVCKkOQx",
        "outputId": "d7543420-67bc-4d1c-9658-b1ece1ee1f92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[122.9445,  98.1300, 114.3399,  95.5700, 119.6243, 102.5016, 150.3902,\n",
              "         118.9190],\n",
              "        [ 95.6930,  76.3270,  88.7811,  74.1526,  93.1516,  79.5312, 116.8576,\n",
              "          92.4227],\n",
              "        [112.2869,  89.4369, 104.3041,  87.1116, 109.1998,  93.3173, 137.1531,\n",
              "         108.4709],\n",
              "        [ 94.6003,  75.2705,  87.7044,  73.1604,  91.9579,  78.3324, 115.3166,\n",
              "          91.2332],\n",
              "        [122.1398,  97.1727, 113.4539,  94.3436, 118.0311, 100.6807, 148.3893,\n",
              "         117.5325],\n",
              "        [103.3264,  82.2776,  95.5361,  79.3966,  99.9396,  84.8448, 125.2358,\n",
              "          99.2093],\n",
              "        [152.2400, 121.3286, 141.1500, 117.5810, 147.5880, 125.8220, 185.2449,\n",
              "         146.6374],\n",
              "        [119.7703,  95.4603, 111.1152,  92.5885, 116.1174,  99.0762, 145.8140,\n",
              "         115.4140]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Attention: Each column in 'Q' is trying to ask a question, and when we calculate K' dot Q, the closely matching Q and K vectors result in large \"attention\" outputs\n",
        "# Each column in attention matrix A tells how that other tokens in the input attend to this token in the input\n",
        "A = torch.einsum('ij, jk -> ik', torch.transpose(K, 0, 1), Q)\n",
        "\n",
        "# TODO: To avoid later tokens to influence earlier tokens (acausal behaviour), we mask the lower diagonal values in A, so that every key with index higher than query index is set to -INF, so that softmax calculation is not impacted\n",
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5bzOqxonSe5",
        "outputId": "0ae5f52d-2ace-4c59-c9ee-863b84be61d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we perform softmax on A to convert each column of A, to find the most likelihood\n",
        "A = nn.functional.softmax(A / sqrt(M), dim=0)\n",
        "A.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2jk0JtPzrP-",
        "outputId": "22026f91-04b7-44b4-8f1f-889052aa86ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now we adjust each input embedding using the self-attention calculated above, to add context information: https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6&t=790s\n",
        "# Conceptually, we use a D X D matrix to generate linearly transformed input embedding, and then dot product with attention A to find required changes in input embedding to add context\n",
        "# Bit it can be more efficiently done by using a product of two weight matrix:\n",
        "\n",
        "WVu = torch.rand((D, M))\n",
        "WVd = torch.rand((M, D))\n",
        "\n",
        "# First, we down-size input embedding from D to M\n",
        "Vd = torch.einsum('ij, jk -> ik', WVd, I)\n",
        "Vd.size()\n",
        "\n",
        "# Then, we up-size Vd back to D space\n",
        "V = torch.einsum('ij, jk -> ik', WVu, Vd)\n",
        "V.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olfgH0nHD3g9",
        "outputId": "c6e07dc2-9ecd-44ea-b5d7-c6a2b39c6940"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, we perform V x A to get change in embeddings: essentially, A gives the weight for each vector, and we average weighted value vectors to get change needed\n",
        "dI = torch.einsum('ij, jk -> ik', V, A)\n",
        "dI.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XMuTcuYbyrB",
        "outputId": "475a8a66-71e2-44e2-b4db-6a0222d31787"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([6, 8])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# And add dI to input embedding to get transformed output embedding\n",
        "O = I + dI\n",
        "O.size()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
